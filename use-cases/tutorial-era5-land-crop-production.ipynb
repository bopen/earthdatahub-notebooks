{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f185885a-7244-4fe6-9894-5d86952097c9",
      "metadata": {
        "id": "f185885a-7244-4fe6-9894-5d86952097c9"
      },
      "source": [
        "# How to work with ERA5 land hourly and Eurostat data on Earth Data Hub\n",
        "### Forecast of crop production"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da89911-e501-4c9b-8ca9-daf58307df40",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-05T16:27:30.942568Z",
          "iopub.status.busy": "2024-01-05T16:27:30.941820Z",
          "iopub.status.idle": "2024-01-05T16:27:30.953114Z",
          "shell.execute_reply": "2024-01-05T16:27:30.950996Z",
          "shell.execute_reply.started": "2024-01-05T16:27:30.942515Z"
        },
        "jp-MarkdownHeadingCollapsed": true,
        "id": "2da89911-e501-4c9b-8ca9-daf58307df40"
      },
      "source": [
        "***\n",
        "This notebook will provide you guidance on how to access and use the [`reanalysis-era5-land-no-antartica-v0.zarr`](https://earthdatahub.com/collections/era5/datasets/reanalysis-era5-land) dataset on Earth Data Hub in combination with Eurostat crop production.\n",
        "\n",
        "Eurostat crop production is available for NUTS levels 0, 1, 2 and starting with the year 2000. The time lag for updates to the database is about two years. Detailed data on NUTS level 2 stop with 2015.\n",
        "\n",
        "The crops of interest here are Cereals (excluding rice) for the production of grain (including seed) for which annual yield in t/ha will be used.\n",
        "\n",
        "In this tutorial, we try to use air temperature and precipitation as a predictor for annual crop yields.\n",
        "\n",
        "The goal is to train and evaluate a neural network to predict annual crop yields from air temperature and precipitation.\n",
        "\n",
        "\n",
        "## What you will learn:\n",
        "\n",
        "* how to access and preview the dataset\n",
        "* select and reduce the data\n",
        "* set up and train a neural network\n",
        "* plot the results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations for Google Colab\n",
        "\n",
        "Install required packages for Google Colab."
      ],
      "metadata": {
        "id": "zPvHID2FEfKv"
      },
      "id": "zPvHID2FEfKv"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zarr\n",
        "!pip install s3fs==2023.6.0\n",
        "!pip install cartopy==0.21.0"
      ],
      "metadata": {
        "id": "0vgeTCAsg3Xm"
      },
      "id": "0vgeTCAsg3Xm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Software Requirements\n",
        "\n",
        "Load required packages."
      ],
      "metadata": {
        "id": "m3XNUUEIfoI0"
      },
      "id": "m3XNUUEIfoI0"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_errorimport scipy.stats\n",
        "from scipy.stats import pearsonr\n",
        "from tqdm import tqdm\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from cartopy import crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "eKIYYinFfrMP"
      },
      "id": "eKIYYinFfrMP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "46089bea-b63e-451b-9c62-be3538440cd5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T18:02:14.626875Z",
          "iopub.status.busy": "2024-01-08T18:02:14.608970Z",
          "iopub.status.idle": "2024-01-08T18:02:14.662440Z",
          "shell.execute_reply": "2024-01-08T18:02:14.644714Z",
          "shell.execute_reply.started": "2024-01-08T18:02:14.626767Z"
        },
        "id": "46089bea-b63e-451b-9c62-be3538440cd5"
      },
      "source": [
        "## Data access and preview\n",
        "***\n",
        "\n",
        "Xarray and Dask work together following a lazy principle. This means when you access and manipulate a Zarr store the data is in not immediately downloaded and loaded in memory. Instead, Dask constructs a task graph that represents the operations to be performed. A smart user will reduce the amount of data that needs to be downloaded before the computation takes place (e.g., when the `.compute()` or `.plot()` methods are called).\n",
        "\n",
        "To preview the data, only the dataset metadata must be downloaded. Xarray does this automatically:\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data access with HTTPS:"
      ],
      "metadata": {
        "id": "0-FQPxtlToGC"
      },
      "id": "0-FQPxtlToGC"
    },
    {
      "cell_type": "code",
      "source": [
        "# import with https\n",
        "\n",
        "dataset = \"https://earthdatahub.com/stores/ecmwf-era5-land/reanalysis-era5-land-no-antartica-v1.zarr\"\n",
        "ds_era5 = xr.open_dataset(\n",
        "    dataset,\n",
        "    chunks={},\n",
        "    engine=\"zarr\",\n",
        "    storage_options={\"client_kwargs\": {\"trust_env\": \"true\"}},\n",
        ")\n",
        "ds_era5"
      ],
      "metadata": {
        "id": "6ZvS2Z4P1NvA"
      },
      "id": "6ZvS2Z4P1NvA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eurostat data"
      ],
      "metadata": {
        "id": "xzbMRlmxHvIo"
      },
      "id": "xzbMRlmxHvIo"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"https://earthdatahub.com/stores/eurostat/apro_cpshr-20000101-20240101.zarr\"\n",
        "\n",
        "ds_eurostat = xr.open_dataset(\n",
        "    dataset,\n",
        "    engine=\"zarr\",\n",
        "    chunks={}\n",
        ")\n",
        "\n",
        "ds_eurostat"
      ],
      "metadata": {
        "id": "dQA76howHyhc"
      },
      "id": "dQA76howHyhc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "19baaf52-2132-40ae-a5b4-596c439be593",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "19baaf52-2132-40ae-a5b4-596c439be593"
      },
      "source": [
        "## Working with data\n",
        "\n",
        "Datasets on EDH are typically very large and remotely hosted. Typical use cases imply a selection of the data followed by one or more reduction steps to be performed in a local or distributed Dask environment.\n",
        "\n",
        "The structure of a workflow that uses EDH data looks like this:\n",
        "1. data selection\n",
        "2. data reduction\n",
        "3. (optional) visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapping longitudes from 0, 360 to -180, 180"
      ],
      "metadata": {
        "id": "BSusxzPmVYx7"
      },
      "id": "BSusxzPmVYx7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a00e9f1-9df0-4cce-8144-b40b8e1711e5",
      "metadata": {
        "id": "4a00e9f1-9df0-4cce-8144-b40b8e1711e5"
      },
      "outputs": [],
      "source": [
        "# wrap longitudes\n",
        "if 0:\n",
        "  lon = ds_era5['longitude']\n",
        "  ds_era5 = ds_era5.assign(**lon: np.where(lon > 180, lon - 360, lon))\n",
        "  # sort the data\n",
        "  ds_era5 = ds_era5.reindex({ 'longitude' : np.sort(ds_era5['longitude'])})\n",
        "else:\n",
        "  lon_name = 'longitude'  # whatever name is in the data\n",
        "  ds = ds_era5\n",
        "\n",
        "  # Adjust lon values to make sure they are within (-180, 180)\n",
        "  ds['_longitude_adjusted'] = xr.where(\n",
        "    ds[lon_name] > 180,\n",
        "    ds[lon_name] - 360,\n",
        "    ds[lon_name])\n",
        "\n",
        "  # reassign the new coords to as the main lon coords\n",
        "  # and sort DataArray using new coordinate values\n",
        "  ds = (\n",
        "    ds\n",
        "    .swap_dims({lon_name: '_longitude_adjusted'})\n",
        "    .sel(**{'_longitude_adjusted': sorted(ds._longitude_adjusted)})\n",
        "    .drop(lon_name))\n",
        "\n",
        "  ds = ds.rename({'_longitude_adjusted': lon_name})\n",
        "\n",
        "ds_era5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94463fdc-e3ba-4da0-ac7c-49abe102655a",
      "metadata": {
        "id": "94463fdc-e3ba-4da0-ac7c-49abe102655a"
      },
      "source": [
        "## Air temperature anomaly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c62f22-abe0-414b-b76a-850663785b5d",
      "metadata": {
        "id": "c6c62f22-abe0-414b-b76a-850663785b5d"
      },
      "source": [
        "### 1. Data selection\n",
        "\n",
        "From the original dataset we extract the air temperature (variable `t2m`) and perform a geographical selection corresponding to Europe. This greatly reduces the amount of data that will be downloaded from EDH.\n",
        "\n",
        "The Eurostat dataset used here starts with the year 2000, while the ERA5 data start with 1940, thus we select only the time period from 2000 onwards.\n",
        "\n",
        "Note that latitudes are decreasing from 90 to -90 degrees and longitudes are increasing from 0 to 360 degrees. Longitudes need to be wrapped to -180, 180."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert from Kelvin to Celsius."
      ],
      "metadata": {
        "id": "LfFaMt0AEadA"
      },
      "id": "LfFaMt0AEadA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select temperature (t2m) and precipitation (tp)"
      ],
      "metadata": {
        "id": "SROfkzFpR5cP"
      },
      "id": "SROfkzFpR5cP"
    },
    {
      "cell_type": "code",
      "source": [
        "# geographical selection\n",
        "ds_t2m = ds_era5.t2m.sel(**{\"latitude\": slice(72, 36), \"longitude\": slice(-10, 32)})\n",
        "# appropriate time period\n",
        "ds_t2m = ds_t2m.sel(valid_time=slice(\"2000-01-01\", \"2024-12-31\"))\n",
        "# convert from Kelvin to Celsius\n",
        "ds_t2m = ds_t2m.astype(\"float32\") - 273.15\n",
        "ds_t2m.attrs[\"units\"] = \"C\"\n",
        "ds_t2m"
      ],
      "metadata": {
        "id": "viISg5eeR-VV"
      },
      "id": "viISg5eeR-VV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a77f23ef-f928-4e8b-b39a-60b067abd4b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T16:15:36.087023Z",
          "iopub.status.busy": "2024-01-12T16:15:36.085688Z",
          "iopub.status.idle": "2024-01-12T16:15:36.103549Z",
          "shell.execute_reply": "2024-01-12T16:15:36.100839Z",
          "shell.execute_reply.started": "2024-01-12T16:15:36.086965Z"
        },
        "id": "a77f23ef-f928-4e8b-b39a-60b067abd4b3"
      },
      "source": [
        "At this point, no data has been downloaded yet, nor loaded in memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e9f5e0-e6e2-4acc-9bd6-f110e3225a4b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T12:06:15.865654Z",
          "iopub.status.busy": "2024-01-08T12:06:15.865172Z",
          "iopub.status.idle": "2024-01-08T12:06:15.922646Z",
          "shell.execute_reply": "2024-01-08T12:06:15.913684Z",
          "shell.execute_reply.started": "2024-01-08T12:06:15.865616Z"
        },
        "id": "89e9f5e0-e6e2-4acc-9bd6-f110e3225a4b"
      },
      "source": [
        "### 2. Data reduction\n",
        "\n",
        "We compute the monthly air temperature averages in the selected region over the reference period:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly averages:"
      ],
      "metadata": {
        "id": "3_BnAWNPblrd"
      },
      "id": "3_BnAWNPblrd"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_t2m_monthly = ds_t2m.resample(valid_time=\"1M\").mean(dim=\"valid_time\")"
      ],
      "metadata": {
        "id": "KHfQw5tmboP-"
      },
      "id": "KHfQw5tmboP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean temperature over the whole reference period:"
      ],
      "metadata": {
        "id": "s75iqQo9cZ1r"
      },
      "id": "s75iqQo9cZ1r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd14620c-f54d-4a97-9049-8c753ed87804",
      "metadata": {
        "id": "fd14620c-f54d-4a97-9049-8c753ed87804"
      },
      "outputs": [],
      "source": [
        "ds_t2m_mean = ds_t2m.mean(dim=\"valid_time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long-term monthly averages"
      ],
      "metadata": {
        "id": "Cf98G06uKomQ"
      },
      "id": "Cf98G06uKomQ"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_t2m_monthly_lt = ds_t2m_monthly.groupby(\"time.month\").mean(\"valid_time\")"
      ],
      "metadata": {
        "id": "uuwarH-iKrVF"
      },
      "id": "uuwarH-iKrVF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly anomalies"
      ],
      "metadata": {
        "id": "G0IKvVOQLFi_"
      },
      "id": "G0IKvVOQLFi_"
    },
    {
      "cell_type": "code",
      "source": [
        "#climatology = ds.groupby(\"time.month\").mean(\"valid_time\")\n",
        "#anomalies = ds.groupby(\"time.month\") - climatology\n",
        "\n",
        "climatology = ds_t2m_monthly.groupby(\"time.month\").mean(\"valid_time\")\n",
        "anomalies = ds_t2m_monthly.groupby(\"time.month\") - climatology\n"
      ],
      "metadata": {
        "id": "YUqN_VLdLHjk"
      },
      "id": "YUqN_VLdLHjk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Growing season from May to September: anomalies"
      ],
      "metadata": {
        "id": "Av8Wd_RsNZxD"
      },
      "id": "Av8Wd_RsNZxD"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_t2m_anom_season = anomalies.sel(time=anomalies.time.dt.month.isin([5, 6, 7, 8, 9]))"
      ],
      "metadata": {
        "id": "VliGF9dYNduL"
      },
      "id": "VliGF9dYNduL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averages for each year"
      ],
      "metadata": {
        "id": "pakTzVipN8Ad"
      },
      "id": "pakTzVipN8Ad"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_t2m_anom_year = ds_t2m_anom_season.groupby(\"time.year\")\n",
        "\n",
        "# or\n",
        "\n",
        "ds_t2m_anom_year = ds_t2m_anom_season.resample(valid_time=\"1Y\").mean(dim=\"valid_time\")"
      ],
      "metadata": {
        "id": "KklA1A_8OEeK"
      },
      "id": "KklA1A_8OEeK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "61b914ed-f3a0-4696-9c39-49877c65d553",
      "metadata": {
        "id": "61b914ed-f3a0-4696-9c39-49877c65d553"
      },
      "source": [
        "After that, we can compute the monthly temperature anomalies in the same area. Calling `compute()` on the result will trigger the download and computation, needed to load the data to the neural network.\n",
        "\n",
        "We can mesure the time it takes, should be about 4 min:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880364f0-6026-4a1f-8e8f-7a25fd49aaad",
      "metadata": {
        "id": "880364f0-6026-4a1f-8e8f-7a25fd49aaad"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "ds_t2m_anom_year = ds_t2m_anom_year.compute()\n",
        "ds_t2m_anom_year"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precipitation anomaly\n",
        "\n",
        "Same procedure as for temperature."
      ],
      "metadata": {
        "id": "UqdgOV9PVQ1L"
      },
      "id": "UqdgOV9PVQ1L"
    },
    {
      "cell_type": "code",
      "source": [
        "# geographical selection\n",
        "ds_tp = ds_era5.tp.sel(**{\"latitude\": slice(72, 36), \"longitude\": slice(-10, 32)})\n",
        "# appropriate time period\n",
        "ds_tp = ds_tp.sel(valid_time=slice(\"2000-01-01\", \"2024-12-31\"))\n",
        "# convert from meters to millimeters\n",
        "ds_tp = ds_tp.astype(\"float32\") * 1000.0\n",
        "ds_tp"
      ],
      "metadata": {
        "id": "Sh-2dXJDR_bJ"
      },
      "id": "Sh-2dXJDR_bJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly sums"
      ],
      "metadata": {
        "id": "PrLf30Y9VwTz"
      },
      "id": "PrLf30Y9VwTz"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tp_monthly = ds_tp.resample(valid_time=\"1M\").reduce(np.sum)"
      ],
      "metadata": {
        "id": "dFDzCcpCVV4J"
      },
      "id": "dFDzCcpCVV4J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long-term monthly averages"
      ],
      "metadata": {
        "id": "yvy-Be1tWtzd"
      },
      "id": "yvy-Be1tWtzd"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tp_monthly_lt = ds_tp_monthly.groupby(\"time.month\").mean(\"valid_time\")"
      ],
      "metadata": {
        "id": "-OIZznqsWwEB"
      },
      "id": "-OIZznqsWwEB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly anomalies"
      ],
      "metadata": {
        "id": "W11hk8L6W31H"
      },
      "id": "W11hk8L6W31H"
    },
    {
      "cell_type": "code",
      "source": [
        "#climatology = ds.groupby(\"time.month\").mean(\"valid_time\")\n",
        "#anomalies = ds.groupby(\"time.month\") - climatology\n",
        "\n",
        "climatology = ds_tp_monthly.groupby(\"time.month\").mean(\"valid_time\")\n",
        "anomalies = ds_tp_monthly.groupby(\"time.month\") - climatology\n"
      ],
      "metadata": {
        "id": "c5_D1unMW5-D"
      },
      "id": "c5_D1unMW5-D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Growing season from May to September: anomalies"
      ],
      "metadata": {
        "id": "uf97hJINXCQW"
      },
      "id": "uf97hJINXCQW"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tp_anom_season = anomalies.sel(time=anomalies.time.dt.month.isin([5, 6, 7, 8, 9]))"
      ],
      "metadata": {
        "id": "Wr_e5Xp7XDU4"
      },
      "id": "Wr_e5Xp7XDU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averages for each year"
      ],
      "metadata": {
        "id": "LQ3ba0-vXPfN"
      },
      "id": "LQ3ba0-vXPfN"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tp_anom_year = ds_tp_anom_season.groupby(\"time.year\")\n",
        "\n",
        "# or\n",
        "\n",
        "ds_tp_anom_year = ds_tp_anom_season.resample(valid_time=\"1Y\").mean(dim=\"valid_time\")"
      ],
      "metadata": {
        "id": "h53_ybcKXRnr"
      },
      "id": "h53_ybcKXRnr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, we can compute the monthly precipitation anomalies in the same area. Calling compute() on the result will trigger the download and computation, needed to load the data to the neural network.\n",
        "\n",
        "We can mesure the time it takes, should be about 4 min:\n"
      ],
      "metadata": {
        "id": "PZNY3Um_XpBo"
      },
      "id": "PZNY3Um_XpBo"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "ds_tp_anom_year = ds_tp_anom_year.compute()\n",
        "ds_tp_anom_year"
      ],
      "metadata": {
        "id": "dyD7D3sLXqiq"
      },
      "id": "dyD7D3sLXqiq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crop yields\n",
        "\n",
        "Annual crop yields on NUTS2 level are calculated as t/ha from harvested production and harvested area."
      ],
      "metadata": {
        "id": "ea5OIRbTYciX"
      },
      "id": "ea5OIRbTYciX"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "ds_eurostat = ds_eurostat.sel(**{\"latitude\": slice(72, 36), \"longitude\": slice(-10, 32)})\n",
        "ds_eurostat = ds_eurostat.assign(cropyield = lambda x: x.L2_A_C1000_PR_HU_EU / x.L2_A_C1000_AR)\n",
        "ds_cropyield = ds_eurostat.cropyield\n",
        "ds_cropyield.compute()\n",
        "ds_cropyield"
      ],
      "metadata": {
        "id": "6YBFle-uZHKD"
      },
      "id": "6YBFle-uZHKD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f524e79f-31cd-436e-bfca-bf7b9bb4b54c",
      "metadata": {
        "id": "f524e79f-31cd-436e-bfca-bf7b9bb4b54c"
      },
      "source": [
        "### 3. Visualization\n",
        "We can plot crop yields for a given year, e.g. 2015, on a map.\n",
        "\n",
        "**! This crashes the Google Colab session !**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f1282f4-a7ca-4517-8cb3-4fa2793f1e11",
      "metadata": {
        "id": "1f1282f4-a7ca-4517-8cb3-4fa2793f1e11"
      },
      "outputs": [],
      "source": [
        "ds_cropyield_2015 = ds_cropyield.sel(time=[\"2015-01-01\"])\n",
        "\n",
        "_, ax = plt.subplots(\n",
        "    figsize=(6, 6),\n",
        "    subplot_kw={\"projection\":  ccrs.Miller()},\n",
        ")\n",
        "ds_cropyield_2015.plot(\n",
        "    ax=ax,\n",
        "    cmap=\"Blues\",\n",
        "    transform=ccrs.PlateCarree(),\n",
        "    cbar_kwargs={\"orientation\": \"horizontal\", \"pad\": 0.05, \"aspect\": 40, \"label\": \"Sea Surface Height anomaly [m]\"},\n",
        ")\n",
        "ax.coastlines()\n",
        "ax.add_feature(cfeature.BORDERS)\n",
        "ax.set_title(\"Crop yields 2015\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation for AI"
      ],
      "metadata": {
        "id": "9LhrUqKiOlU2"
      },
      "id": "9LhrUqKiOlU2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already computed monthly temperature and precipitation anomalies from the ERA5 dataset and now need to train a network."
      ],
      "metadata": {
        "id": "V1KgK8dST8PL"
      },
      "id": "V1KgK8dST8PL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions to manage input data:"
      ],
      "metadata": {
        "id": "GqtPJ1RySFag"
      },
      "id": "GqtPJ1RySFag"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EE02i6wD_9L"
      },
      "source": [
        "# Scaffold code to load in data.  This code cell is mostly data wrangling\n",
        "\n",
        "def assemble_predictors_predictands(ds_t2m, ds_tp, ds_crop,\n",
        "                                    start_date, end_date,\n",
        "                                    use_pca=False, n_components=32):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      ds_t2m            xarray : input xarray dataset with temperature\n",
        "      ds_tp             xarray : input xarray dataset with precipitation\n",
        "      ds_crop           xarray : input xarray dataset with crop yields\n",
        "      start_date           str : the start date from which to extract sst\n",
        "      end_date             str : the end date\n",
        "      use_pca             bool : whether or not to apply principal components\n",
        "                                 analysis to the sst field\n",
        "      n_components         int : the number of components to use for PCA\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "      Returns a tuple of the predictors (np array of temperature and precipitation anomalies)\n",
        "      and the predictands (np array of crop yields).\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  t2m = ds_t2m.sel(valid_time=slice(start_date, end_date))\n",
        "  tp = ds_tp.sel(valid_time=slice(start_date, end_date))\n",
        "  cropyields = ds_crop.sel(time=slice(start_date, end_date))\n",
        "\n",
        "  num_samples = sst.shape[0]\n",
        "  # t2m and tp are (num_samples, lat, lon) arrays\n",
        "  # the line below combines them to (num_samples, num_params, lat, lon)\n",
        "  # with num_params = 2\n",
        "  #sst = np.stack([sst.values[n-num_input_time_steps:n] for n in range(num_input_time_steps,\n",
        "  #                                                            num_samples+1)])\n",
        "\n",
        "  combined_arr = np.stack((t2m, tp), axis=1)\n",
        "\n",
        "  num_samples = combined_arr.shape[0]\n",
        "\n",
        "  combined_arr[np.isnan(combined_arr)] = 0\n",
        "  X = combined_arr\n",
        "\n",
        "  cropyields[np.isnan(cropyields)] = 0\n",
        "  y = cropyields\n",
        "\n",
        "  return X.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "\n",
        "class ERA5Dataset(Dataset):\n",
        "    def __init__(self, predictors, predictands):\n",
        "        self.predictors = predictors\n",
        "        self.predictands = predictands\n",
        "        assert self.predictors.shape[0] == self.predictands.shape[0], \\\n",
        "               \"The number of predictors must equal the number of predictands!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.predictors.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.predictors[idx], self.predictands[idx]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "_EE02i6wD_9L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a Simple Convolutional Neural Network to model crop yields\n",
        "\n",
        "Let's define a simple convolutional neural network architecture.  This architecture has one convolutional layer, followed by a pooling layer, followed by another convolutional layer, followed by two transposed convolutional layers.  The output of the final layer is a 2-D array, since we are trying to model one value for each pixel: the crop yield."
      ],
      "metadata": {
        "id": "XrEZRMItafsC"
      },
      "id": "XrEZRMItafsC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the network"
      ],
      "metadata": {
        "id": "hbUd0zDwfEBG"
      },
      "id": "hbUd0zDwfEBG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7g4htyetytL"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_params=2, print_feature_dimension=False):\n",
        "        \"\"\"\n",
        "        inputs\n",
        "        -------\n",
        "            num_params                  (int) : the number of parameters\n",
        "                                                in the predictor\n",
        "            print_feature_dimension    (bool) : whether or not to print\n",
        "                                                out the dimension of the features\n",
        "                                                extracted from the conv layers\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_params, 15, 3)\n",
        "        self.pool = nn.MaxPool2d(3, 3)\n",
        "        self.conv2 = nn.Conv2d(15, 32, 3)\n",
        "        self.print_layer = Print()\n",
        "\n",
        "        # TIP: print out the dimension of the extracted features from\n",
        "        # the conv layers for setting the dimension of the linear layer!\n",
        "        # Using the print_layer, we find that the dimensions are\n",
        "        # (batch_size, 32, 18, 108)\n",
        "        self.deconv1 = nn.ConvTranspose2d(32, 15, 3)\n",
        "        self.deconv2 = nn.ConvTranspose2d(15, 1, 3)\n",
        "        self.print_feature_dimension = print_feature_dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        if self.print_feature_dimension:\n",
        "          x = self.print_layer(x)\n",
        "        # adjust to dimensions of fc1\n",
        "        #x = x.view(-1, 32 * 8 * 48)\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        return x\n",
        "\n",
        "class Print(nn.Module):\n",
        "    \"\"\"\n",
        "    This class prints out the size of the features\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        print(x.size())\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "S7g4htyetytL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's define a method that trains our neural network."
      ],
      "metadata": {
        "id": "RZb5SQqXbwyX"
      },
      "id": "RZb5SQqXbwyX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIgp39lRsGoW"
      },
      "source": [
        "def train_network(net, criterion, optimizer, trainloader, testloader,\n",
        "                  experiment_name, num_epochs=40):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      net               (nn.Module)   : the neural network architecture\n",
        "      criterion         (nn)          : the loss function (i.e. root mean squared error)\n",
        "      optimizer         (torch.optim) : the optimizer to use update the neural network\n",
        "                                        architecture to minimize the loss function\n",
        "      trainloader       (torch.utils.data.DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the train dataset\n",
        "      testloader        (torch.utils.data. DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the test dataset\n",
        "  outputs\n",
        "  -------\n",
        "      predictions (np.array), and saves the trained neural network as a .pt file\n",
        "  \"\"\"\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  net = net.to(device)\n",
        "  best_loss = np.infty\n",
        "  train_losses, test_losses = [], []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for mode, data_loader in [('train', trainloader), ('test', testloader)]:\n",
        "      # Set the model to train mode to allow its weights to be updated\n",
        "      # while training\n",
        "      if mode == 'train':\n",
        "        net.train()\n",
        "\n",
        "      # Set the model to eval model to prevent its weights from being updated\n",
        "      # while testing\n",
        "      elif mode == 'test':\n",
        "        net.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(data_loader):\n",
        "          # get a mini-batch of predictors and predictands\n",
        "          batch_predictors, batch_predictands = data\n",
        "          batch_predictands = batch_predictands.to(device)\n",
        "          batch_predictors = batch_predictors.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # calculate the predictions of the current neural network\n",
        "          predictions = net(batch_predictors).squeeze()\n",
        "\n",
        "          # quantify the quality of the predictions using a\n",
        "          # loss function (aka criterion) that is differentiable\n",
        "          loss = criterion(predictions, batch_predictands)\n",
        "\n",
        "          if mode == 'train':\n",
        "            # the 'backward pass: calculates the gradients of each weight\n",
        "            # of the neural network with respect to the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # the optimizer updates the weights of the neural network\n",
        "            # based on the gradients calculated above and the choice\n",
        "            # of optimization algorithm\n",
        "            optimizer.step()\n",
        "\n",
        "          # Save the model weights that have the best performance!\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      if running_loss < best_loss and mode == 'test':\n",
        "          best_loss = running_loss\n",
        "          torch.save(net, '{}.pt'.format(experiment_name))\n",
        "      print('{} Set: Epoch {:02d}. loss: {:3f}'.format(mode, epoch+1, \\\n",
        "                                            running_loss/len(data_loader)))\n",
        "      if mode == 'train':\n",
        "          train_losses.append(running_loss/len(data_loader))\n",
        "      else:\n",
        "          test_losses.append(running_loss/len(data_loader))\n",
        "\n",
        "  net = torch.load('{}.pt'.format(experiment_name))\n",
        "  net.eval()\n",
        "  net.to(device)\n",
        "\n",
        "  # the remainder of this function calculates the predictions of the best\n",
        "  # saved model\n",
        "  predictions = np.asarray([])\n",
        "  for i, data in enumerate(testloader):\n",
        "    batch_predictors, batch_predictands = data\n",
        "    batch_predictands = batch_predictands.to(device)\n",
        "    batch_predictors = batch_predictors.to(device)\n",
        "\n",
        "    batch_predictions = net(batch_predictors).squeeze()\n",
        "    # Edge case: if there is 1 item in the batch, batch_predictions becomes a float\n",
        "    # not a Tensor. the if statement below converts it to a Tensor\n",
        "    # so that it is compatible with np.concatenate\n",
        "    if len(batch_predictions.size()) == 0:\n",
        "      batch_predictions = torch.Tensor([batch_predictions])\n",
        "    predictions = np.concatenate([predictions, batch_predictions.detach().cpu().numpy()])\n",
        "  return predictions, train_losses, test_losses\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "JIgp39lRsGoW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actual training"
      ],
      "metadata": {
        "id": "MBVvnICufT0y"
      },
      "id": "MBVvnICufT0y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for the network and train it.\n",
        "\n",
        "The earliest possible start date is 2000-01-01, the latest possible end date is 2015-12-31."
      ],
      "metadata": {
        "id": "EnZDaZBxde_d"
      },
      "id": "EnZDaZBxde_d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USvje5kouCGb"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Assemble numpy arrays corresponding to predictors and predictands\n",
        "train_start_date = '2000-01-01'\n",
        "train_end_date = '2011-12-31'\n",
        "\n",
        "test_start_date = '2012-01-01'\n",
        "test_end_date = '2015-12-31'\n",
        "\n",
        "train_predictors, train_predictands = assemble_predictors_predictands(ds_t2m_anom_year, ds_tp_anom_year, ds_cropyield,\n",
        "                                                                      train_start_date, train_end_date)\n",
        "\n",
        "print(\"train_predictors: %d\" % train_predictors.shape[0])\n",
        "print(\"train_predictands: %d\" % train_predictands.shape[0])\n",
        "\n",
        "test_predictors, test_predictands = assemble_predictors_predictands(ds_t2m_anom_year, ds_tp_anom_year, ds_cropyield,\n",
        "                                                                    test_start_date, test_end_date)\n",
        "\n",
        "print(\"test_predictors: %d\" % test_predictors.shape[0])\n",
        "print(\"test_predictands: %d\" % test_predictands.shape[0])\n",
        "\n",
        "# Convert the numpy ararys into ERA5Dataset, which is a subset of the\n",
        "# torch.utils.data.Dataset class.  This class is compatible with\n",
        "# the torch dataloader, which allows for data loading for a CNN\n",
        "train_dataset = ERA5Dataset(train_predictors, train_predictands)\n",
        "test_dataset = ERA5Dataset(test_predictors, test_predictands)\n",
        "\n",
        "# Create a torch.utils.data.DataLoader from the ERA5Dataset() created earlier!\n",
        "# the similarity between the name DataLoader and Dataset in the pytorch API is unfortunate...\n",
        "trainloader = DataLoader(train_dataset, batch_size=2)\n",
        "testloader = DataLoader(test_dataset, batch_size=2)\n",
        "net = CNN(num_params=2, print_feature_dimension=False)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "# train the model and make predictions for the test time period\n",
        "experiment_name = \"twolayerCNN_{}_{}\".format(train_start_date, train_end_date)\n",
        "predictions, train_losses, test_losses = train_network(net, nn.MSELoss(),\n",
        "                  optimizer, trainloader, testloader, experiment_name, num_epochs=40)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "USvje5kouCGb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results of NN training"
      ],
      "metadata": {
        "id": "IAs-MbrXfX_Y"
      },
      "id": "IAs-MbrXfX_Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot train and test losses:"
      ],
      "metadata": {
        "id": "dkzL7lZCb5by"
      },
      "id": "dkzL7lZCb5by"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Performance of {} Neural Network During Training'.format(experiment_name))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pg6OyWkhb8Mf"
      },
      "id": "Pg6OyWkhb8Mf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If test loss does not look satisfactory, try reducing the number of parameters of the network. You could define your own network architecture, which uses a different number of parameters.\n",
        "\n",
        "Alternatively, try increasing the number of training samples by using a longer time period or a larger area."
      ],
      "metadata": {
        "id": "JvC0xHkzcIbD"
      },
      "id": "JvC0xHkzcIbD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show predictions:"
      ],
      "metadata": {
        "id": "hU6bkIQGc5nL"
      },
      "id": "hU6bkIQGc5nL"
    },
    {
      "cell_type": "code",
      "source": [
        "corr, _ = pearsonr(test_predictands, predictions)\n",
        "rmse = mean_squared_error(test_predictands, predictions) ** 0.5\n",
        "\n",
        "print(\"RMSE: {:.2f}\".format(rmse))\n"
      ],
      "metadata": {
        "id": "iu2GlMiQcaXj"
      },
      "id": "iu2GlMiQcaXj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}