{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f185885a-7244-4fe6-9894-5d86952097c9",
      "metadata": {
        "id": "f185885a-7244-4fe6-9894-5d86952097c9"
      },
      "source": [
        "# How to work with ERA5 land hourly and Eurostat data on Earth Data Hub\n",
        "### Forecast of crop production"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da89911-e501-4c9b-8ca9-daf58307df40",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-05T16:27:30.942568Z",
          "iopub.status.busy": "2024-01-05T16:27:30.941820Z",
          "iopub.status.idle": "2024-01-05T16:27:30.953114Z",
          "shell.execute_reply": "2024-01-05T16:27:30.950996Z",
          "shell.execute_reply.started": "2024-01-05T16:27:30.942515Z"
        },
        "jp-MarkdownHeadingCollapsed": true,
        "id": "2da89911-e501-4c9b-8ca9-daf58307df40"
      },
      "source": [
        "***\n",
        "This notebook will provide you guidance on how to access and use the [`reanalysis-era5-land-no-antartica-v0.zarr`](https://earthdatahub.com/collections/era5/datasets/reanalysis-era5-land) dataset on Earth Data Hub in combination with Eurostat crop production.\n",
        "\n",
        "Eurostat crop production is available for NUTS levels 0, 1, 2 and starting with the year 2000. The time lag for updates to the database is about two years. Detailed data on NUTS level 2 stop with 2015.\n",
        "\n",
        "The crops of interest here are Cereals (excluding rice) for the production of grain (including seed) for which annual yield in t/ha will be used.\n",
        "\n",
        "In this tutorial, we try to use air temperature and precipitation as a predictor for annual crop yields.\n",
        "\n",
        "The goal is to train and evaluate a neural network to predict annual crop yields from air temperature and precipitation.\n",
        "\n",
        "\n",
        "## What you will learn:\n",
        "\n",
        "* how to access and preview the dataset\n",
        "* select and reduce the data\n",
        "* set up and train a neural network\n",
        "* plot the results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations for Google Colab\n",
        "\n",
        "Install required packages for Google Colab."
      ],
      "metadata": {
        "id": "zPvHID2FEfKv"
      },
      "id": "zPvHID2FEfKv"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zarr\n",
        "!pip install s3fs==2023.6.0\n",
        "!pip install cartopy==0.21.0"
      ],
      "metadata": {
        "id": "0vgeTCAsg3Xm"
      },
      "id": "0vgeTCAsg3Xm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Software Requirements\n",
        "\n",
        "Load required packages."
      ],
      "metadata": {
        "id": "m3XNUUEIfoI0"
      },
      "id": "m3XNUUEIfoI0"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import scipy.stats\n",
        "from scipy.stats import pearsonr\n",
        "from tqdm import tqdm\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from cartopy import crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "eKIYYinFfrMP"
      },
      "id": "eKIYYinFfrMP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "46089bea-b63e-451b-9c62-be3538440cd5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T18:02:14.626875Z",
          "iopub.status.busy": "2024-01-08T18:02:14.608970Z",
          "iopub.status.idle": "2024-01-08T18:02:14.662440Z",
          "shell.execute_reply": "2024-01-08T18:02:14.644714Z",
          "shell.execute_reply.started": "2024-01-08T18:02:14.626767Z"
        },
        "id": "46089bea-b63e-451b-9c62-be3538440cd5"
      },
      "source": [
        "## Data access and preview\n",
        "***\n",
        "\n",
        "Xarray and Dask work together following a lazy principle. This means when you access and manipulate a Zarr store the data is in not immediately downloaded and loaded in memory. Instead, Dask constructs a task graph that represents the operations to be performed. A smart user will reduce the amount of data that needs to be downloaded before the computation takes place (e.g., when the `.compute()` or `.plot()` methods are called).\n",
        "\n",
        "To preview the data, only the dataset metadata must be downloaded. Xarray does this automatically:\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data access with HTTPS:"
      ],
      "metadata": {
        "id": "0-FQPxtlToGC"
      },
      "id": "0-FQPxtlToGC"
    },
    {
      "cell_type": "code",
      "source": [
        "# import with https\n",
        "\n",
        "# ERA5 land\n",
        "#dataset = \"https://earthdatahub.com/stores/ecmwf-era5-land/reanalysis-era5-land-no-antartica-v1.zarr\"\n",
        "# ERA5 single levels land + ocean\n",
        "dataset = \"https://earthdatahub.com/stores/ecmwf-era5-single-levels/reanalysis-era5-single-levels-v0.zarr\"\n",
        "ds_era5 = xr.open_dataset(\n",
        "    dataset,\n",
        "    chunks={},\n",
        "    engine=\"zarr\",\n",
        "    storage_options={\"client_kwargs\": {\"trust_env\": \"true\"}},\n",
        ")\n",
        "\n",
        "ds_era5"
      ],
      "metadata": {
        "id": "6ZvS2Z4P1NvA"
      },
      "id": "6ZvS2Z4P1NvA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eurostat data, spatial extents should be latitude [26.0, 73.9], longitude [-27.0, 44.9]"
      ],
      "metadata": {
        "id": "xzbMRlmxHvIo"
      },
      "id": "xzbMRlmxHvIo"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"https://earthdatahub.com/stores/eurostat/apro_cpshr-20000101-20240101.zarr\"\n",
        "\n",
        "ds_eurostat = xr.open_dataset(\n",
        "    dataset,\n",
        "    engine=\"zarr\",\n",
        "    chunks={}\n",
        ")\n",
        "\n",
        "ds_eurostat"
      ],
      "metadata": {
        "id": "dQA76howHyhc"
      },
      "id": "dQA76howHyhc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "19baaf52-2132-40ae-a5b4-596c439be593",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "19baaf52-2132-40ae-a5b4-596c439be593"
      },
      "source": [
        "## Working with data\n",
        "\n",
        "Datasets on EDH are typically very large and remotely hosted. Typical use cases imply a selection of the data followed by one or more reduction steps to be performed in a local or distributed Dask environment.\n",
        "\n",
        "The structure of a workflow that uses EDH data looks like this:\n",
        "1. data selection\n",
        "2. data reduction\n",
        "3. (optional) visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Eurostat dataset used here starts with the year 2000, while the ERA5 data start with 1940, thus we select only the time period from 2000 onwards."
      ],
      "metadata": {
        "id": "1PYrl3EO9Bc6"
      },
      "id": "1PYrl3EO9Bc6"
    },
    {
      "cell_type": "code",
      "source": [
        "# appropriate time period\n",
        "ds_era5 = ds_era5.sel(valid_time=slice(\"2000-01-01\", \"2024-12-31\"))"
      ],
      "metadata": {
        "id": "Ba33bb0I9Chp"
      },
      "id": "Ba33bb0I9Chp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that latitudes are decreasing from 90 to -90 degrees and longitudes are increasing from 0 to 360 degrees. Longitudes need to be wrapped to -180, 180."
      ],
      "metadata": {
        "id": "BSusxzPmVYx7"
      },
      "id": "BSusxzPmVYx7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a00e9f1-9df0-4cce-8144-b40b8e1711e5",
      "metadata": {
        "id": "4a00e9f1-9df0-4cce-8144-b40b8e1711e5"
      },
      "outputs": [],
      "source": [
        "# wrap longitudes from 0, 360 to -180, 180\n",
        "\n",
        "# from https://docs.xarray.dev/en/stable/generated/xarray.DataArray.assign_coords.html\n",
        "ds_era5 = ds_era5.assign_coords(longitude=(((ds_era5.longitude + 180) % 360) - 180))\n",
        "\n",
        "#ds_era5.assign_coords(longitude=(xr.where(ds_era5['longitude'] > 180, ds_era5['longitude'] - 360, ds_era5['longitude'])))\n",
        "# sort the data, needed!\n",
        "ds_era5 = ds_era5.reindex({ 'longitude' : np.sort(ds_era5['longitude'])})\n",
        "\n",
        "ds_era5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94463fdc-e3ba-4da0-ac7c-49abe102655a",
      "metadata": {
        "id": "94463fdc-e3ba-4da0-ac7c-49abe102655a"
      },
      "source": [
        "## Air temperature anomaly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c62f22-abe0-414b-b76a-850663785b5d",
      "metadata": {
        "id": "c6c62f22-abe0-414b-b76a-850663785b5d"
      },
      "source": [
        "### 1. Data selection\n",
        "\n",
        "From the original dataset we extract the air temperature (variable `t2m`) and perform a geographical selection corresponding to Europe. This greatly reduces the amount of data that will be downloaded from EDH."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert from Kelvin to Celsius."
      ],
      "metadata": {
        "id": "LfFaMt0AEadA"
      },
      "id": "LfFaMt0AEadA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select temperature (t2m) and precipitation (tp)"
      ],
      "metadata": {
        "id": "SROfkzFpR5cP"
      },
      "id": "SROfkzFpR5cP"
    },
    {
      "cell_type": "code",
      "source": [
        "# geographical selection\n",
        "# Eurostat extent: latitude [26.0, 73.9], longitude [-27.0, 44.9]\n",
        "ds_t2m = ds_era5.t2m.sel(**{\"latitude\": slice(72, 36), \"longitude\": slice(-10, 32)})\n",
        "# convert from Kelvin to Celsius\n",
        "ds_t2m = ds_t2m.astype(\"float32\") - 273.15\n",
        "ds_t2m.attrs[\"units\"] = \"C\"\n",
        "ds_t2m"
      ],
      "metadata": {
        "id": "viISg5eeR-VV"
      },
      "id": "viISg5eeR-VV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a77f23ef-f928-4e8b-b39a-60b067abd4b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T16:15:36.087023Z",
          "iopub.status.busy": "2024-01-12T16:15:36.085688Z",
          "iopub.status.idle": "2024-01-12T16:15:36.103549Z",
          "shell.execute_reply": "2024-01-12T16:15:36.100839Z",
          "shell.execute_reply.started": "2024-01-12T16:15:36.086965Z"
        },
        "id": "a77f23ef-f928-4e8b-b39a-60b067abd4b3"
      },
      "source": [
        "At this point, no data has been downloaded yet, nor loaded in memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e9f5e0-e6e2-4acc-9bd6-f110e3225a4b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T12:06:15.865654Z",
          "iopub.status.busy": "2024-01-08T12:06:15.865172Z",
          "iopub.status.idle": "2024-01-08T12:06:15.922646Z",
          "shell.execute_reply": "2024-01-08T12:06:15.913684Z",
          "shell.execute_reply.started": "2024-01-08T12:06:15.865616Z"
        },
        "id": "89e9f5e0-e6e2-4acc-9bd6-f110e3225a4b"
      },
      "source": [
        "### 2. Data reduction\n",
        "\n",
        "We compute the monthly air temperature averages in the selected region over the reference period."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly averages:"
      ],
      "metadata": {
        "id": "3_BnAWNPblrd"
      },
      "id": "3_BnAWNPblrd"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "ds_t2m_monthly = ds_t2m.resample(valid_time=\"1M\").mean(dim=\"valid_time\")\n",
        "ds_t2m_monthly = ds_t2m_monthly.compute()\n",
        "ds_t2m_monthly"
      ],
      "metadata": {
        "id": "KHfQw5tmboP-"
      },
      "id": "KHfQw5tmboP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long-term monthly averages"
      ],
      "metadata": {
        "id": "Cf98G06uKomQ"
      },
      "id": "Cf98G06uKomQ"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_t2m_monthly_lt = ds_t2m_monthly.groupby(\"valid_time.month\").mean(\"valid_time\")"
      ],
      "metadata": {
        "id": "uuwarH-iKrVF"
      },
      "id": "uuwarH-iKrVF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly anomalies"
      ],
      "metadata": {
        "id": "G0IKvVOQLFi_"
      },
      "id": "G0IKvVOQLFi_"
    },
    {
      "cell_type": "code",
      "source": [
        "#climatology = ds.groupby(\"time.month\").mean(\"valid_time\")\n",
        "#anomalies = ds.groupby(\"time.month\") - climatology\n",
        "\n",
        "climatology = ds_t2m_monthly.groupby(\"valid_time.month\").mean(\"valid_time\")\n",
        "anomalies = ds_t2m_monthly.groupby(\"valid_time.month\") - climatology\n",
        "anomalies"
      ],
      "metadata": {
        "id": "YUqN_VLdLHjk"
      },
      "id": "YUqN_VLdLHjk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Growing season from May to September: anomalies"
      ],
      "metadata": {
        "id": "Av8Wd_RsNZxD"
      },
      "id": "Av8Wd_RsNZxD"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_t2m_anom_season = anomalies.sel(valid_time=anomalies.valid_time.dt.month.isin([5, 6, 7, 8, 9]))"
      ],
      "metadata": {
        "id": "VliGF9dYNduL"
      },
      "id": "VliGF9dYNduL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averages for each year"
      ],
      "metadata": {
        "id": "pakTzVipN8Ad"
      },
      "id": "pakTzVipN8Ad"
    },
    {
      "cell_type": "code",
      "source": [
        "#ds_t2m_anom_year = ds_t2m_anom_season.groupby(\"valid_time.year\")\n",
        "\n",
        "# or\n",
        "\n",
        "ds_t2m_anom_year = ds_t2m_anom_season.resample(valid_time=\"1Y\").mean(dim=\"valid_time\")"
      ],
      "metadata": {
        "id": "KklA1A_8OEeK"
      },
      "id": "KklA1A_8OEeK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "61b914ed-f3a0-4696-9c39-49877c65d553",
      "metadata": {
        "id": "61b914ed-f3a0-4696-9c39-49877c65d553"
      },
      "source": [
        "After that, we can compute the monthly temperature anomalies in the same area. Calling `compute()` on the result will trigger the download and computation, needed to load the data to the neural network.\n",
        "\n",
        "We can mesure the time it takes, should be about 4 min:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880364f0-6026-4a1f-8e8f-7a25fd49aaad",
      "metadata": {
        "id": "880364f0-6026-4a1f-8e8f-7a25fd49aaad"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "ds_t2m_anom_year = ds_t2m_anom_year.compute()\n",
        "ds_t2m_anom_year"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precipitation anomaly\n",
        "\n",
        "Same procedure as for temperature."
      ],
      "metadata": {
        "id": "UqdgOV9PVQ1L"
      },
      "id": "UqdgOV9PVQ1L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data selection"
      ],
      "metadata": {
        "id": "XQ7UddmHraUR"
      },
      "id": "XQ7UddmHraUR"
    },
    {
      "cell_type": "code",
      "source": [
        "# geographical selection\n",
        "ds_tp = ds_era5.tp.sel(**{\"latitude\": slice(72, 36), \"longitude\": slice(-10, 32)})\n",
        "# convert from meters to millimeters\n",
        "ds_tp = ds_tp.astype(\"float32\") * 1000.0\n",
        "ds_tp"
      ],
      "metadata": {
        "id": "Sh-2dXJDR_bJ"
      },
      "id": "Sh-2dXJDR_bJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data reduction\n",
        "\n",
        "Instead of monthly averages, we compute the monthly precipitation sums in the selected region over the reference period.\n",
        "\n",
        "Monthly sums:"
      ],
      "metadata": {
        "id": "PrLf30Y9VwTz"
      },
      "id": "PrLf30Y9VwTz"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "ds_tp_monthly = ds_tp.resample(valid_time=\"1M\").reduce(np.sum)\n",
        "ds_tp_monthly = ds_tp_monthly.compute()\n",
        "ds_tp_monthly"
      ],
      "metadata": {
        "id": "dFDzCcpCVV4J"
      },
      "id": "dFDzCcpCVV4J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long-term monthly averages"
      ],
      "metadata": {
        "id": "yvy-Be1tWtzd"
      },
      "id": "yvy-Be1tWtzd"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tp_monthly_lt = ds_tp_monthly.groupby(\"valid_time.month\").mean(\"valid_time\")"
      ],
      "metadata": {
        "id": "-OIZznqsWwEB"
      },
      "id": "-OIZznqsWwEB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly anomalies"
      ],
      "metadata": {
        "id": "W11hk8L6W31H"
      },
      "id": "W11hk8L6W31H"
    },
    {
      "cell_type": "code",
      "source": [
        "#climatology = ds.groupby(\"time.month\").mean(\"valid_time\")\n",
        "#anomalies = ds.groupby(\"time.month\") - climatology\n",
        "\n",
        "climatology = ds_tp_monthly.groupby(\"valid_time.month\").mean(\"valid_time\")\n",
        "anomalies = ds_tp_monthly.groupby(\"valid_time.month\") - climatology\n"
      ],
      "metadata": {
        "id": "c5_D1unMW5-D"
      },
      "id": "c5_D1unMW5-D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Growing season from May to September: anomalies"
      ],
      "metadata": {
        "id": "uf97hJINXCQW"
      },
      "id": "uf97hJINXCQW"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tp_anom_season = anomalies.sel(valid_time=anomalies.valid_time.dt.month.isin([5, 6, 7, 8, 9]))"
      ],
      "metadata": {
        "id": "Wr_e5Xp7XDU4"
      },
      "id": "Wr_e5Xp7XDU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averages for each year"
      ],
      "metadata": {
        "id": "LQ3ba0-vXPfN"
      },
      "id": "LQ3ba0-vXPfN"
    },
    {
      "cell_type": "code",
      "source": [
        "#ds_tp_anom_year = ds_tp_anom_season.groupby(\"valid_time.year\")\n",
        "\n",
        "# or\n",
        "\n",
        "ds_tp_anom_year = ds_tp_anom_season.resample(valid_time=\"1Y\").mean(dim=\"valid_time\")"
      ],
      "metadata": {
        "id": "h53_ybcKXRnr"
      },
      "id": "h53_ybcKXRnr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, we can compute the monthly precipitation anomalies in the same area. Calling compute() on the result will trigger the download and computation, needed to load the data to the neural network.\n",
        "\n",
        "We can mesure the time it takes, should be about 4 min:\n"
      ],
      "metadata": {
        "id": "PZNY3Um_XpBo"
      },
      "id": "PZNY3Um_XpBo"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "ds_tp_anom_year = ds_tp_anom_year.compute()\n",
        "ds_tp_anom_year"
      ],
      "metadata": {
        "id": "dyD7D3sLXqiq"
      },
      "id": "dyD7D3sLXqiq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crop yields\n",
        "\n",
        "Annual crop yields on NUTS2 level are calculated as t/ha from harvested production and harvested area."
      ],
      "metadata": {
        "id": "ea5OIRbTYciX"
      },
      "id": "ea5OIRbTYciX"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "ds_eurostat = ds_eurostat.sel(**{\"lat\": slice(36, 72), \"lon\": slice(-10, 32)})\n",
        "#  && !np.isnan(x.L2_A_C1000_AR)\n",
        "ds_eurostat = ds_eurostat.assign(cropyield = lambda x: x.L2_A_C1000_PR_HU_EU.where((x.L2_A_C1000_AR != 0) & (np.isnan(x.L2_A_C1000_AR) == False), 0) / x.L2_A_C1000_AR)\n",
        "ds_eurostat"
      ],
      "metadata": {
        "id": "6YBFle-uZHKD"
      },
      "id": "6YBFle-uZHKD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ds_eurostat = ds_eurostat.compute()\n",
        "ds_cropyield = ds_eurostat.cropyield\n",
        "ds_cropyield.compute()\n",
        "ds_cropyield"
      ],
      "metadata": {
        "id": "C7Z-n7UCN5CP"
      },
      "id": "C7Z-n7UCN5CP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f524e79f-31cd-436e-bfca-bf7b9bb4b54c",
      "metadata": {
        "id": "f524e79f-31cd-436e-bfca-bf7b9bb4b54c"
      },
      "source": [
        "### 3. Visualization\n",
        "We can plot crop yields for a given year, e.g. 2015, on a map.\n",
        "\n",
        "**! This crashes the Google Colab session !**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f1282f4-a7ca-4517-8cb3-4fa2793f1e11",
      "metadata": {
        "id": "1f1282f4-a7ca-4517-8cb3-4fa2793f1e11"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  ds_cropyield_2015 = ds_cropyield.sel(time=[\"2015-01-01\"])\n",
        "\n",
        "  _, ax = plt.subplots(\n",
        "      figsize=(6, 6),\n",
        "      subplot_kw={\"projection\":  ccrs.Miller()},\n",
        "  )\n",
        "  ds_cropyield_2015.plot(\n",
        "      ax=ax,\n",
        "      cmap=\"Blues\",\n",
        "      transform=ccrs.PlateCarree(),\n",
        "      cbar_kwargs={\"orientation\": \"horizontal\", \"pad\": 0.05, \"aspect\": 40, \"label\": \"Sea Surface Height anomaly [m]\"},\n",
        "  )\n",
        "  ax.coastlines()\n",
        "  ax.add_feature(cfeature.BORDERS)\n",
        "  ax.set_title(\"Crop yields 2015\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation for AI"
      ],
      "metadata": {
        "id": "9LhrUqKiOlU2"
      },
      "id": "9LhrUqKiOlU2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two xarrays must have the same spatial resolution and extents. Therefore we need to adjust one xarray to the other."
      ],
      "metadata": {
        "id": "W3LA8dgDu8tv"
      },
      "id": "W3LA8dgDu8tv"
    },
    {
      "cell_type": "code",
      "source": [
        "# interpolate ERA5 data to match eurostat data\n",
        "\n",
        "ds_t2m_anom_year_interp = ds_t2m_anom_year.interp(latitude=ds_cropyield[\"lat\"], longitude=ds_cropyield[\"lon\"])\n",
        "ds_tp_anom_year_interp = ds_tp_anom_year.interp(latitude=ds_cropyield[\"lat\"], longitude=ds_cropyield[\"lon\"])\n"
      ],
      "metadata": {
        "id": "7oSAKbsYvMKl"
      },
      "id": "7oSAKbsYvMKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_t2m_anom_year_interp"
      ],
      "metadata": {
        "id": "ZH66Z6o3gftZ"
      },
      "id": "ZH66Z6o3gftZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already computed monthly temperature and precipitation anomalies from the ERA5 dataset and now need to train a network."
      ],
      "metadata": {
        "id": "V1KgK8dST8PL"
      },
      "id": "V1KgK8dST8PL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions to manage input data:"
      ],
      "metadata": {
        "id": "GqtPJ1RySFag"
      },
      "id": "GqtPJ1RySFag"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EE02i6wD_9L"
      },
      "source": [
        "# Scaffold code to load in data.  This code cell is mostly data wrangling\n",
        "\n",
        "def assemble_predictors(ds_t2m, ds_tp,\n",
        "                        start_date, end_date):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      ds_t2m            xarray : input xarray dataset with temperature\n",
        "      ds_tp             xarray : input xarray dataset with precipitation\n",
        "      start_date           str : the start date from which to extract sst\n",
        "      end_date             str : the end date\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "      Returns the predictors (np array of temperature and precipitation anomalies)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  t2m = ds_t2m.sel(valid_time=slice(start_date, end_date))\n",
        "  tp = ds_tp.sel(valid_time=slice(start_date, end_date))\n",
        "\n",
        "  # t2m and tp are (num_samples, lat, lon) arrays\n",
        "  # the line below combines them to (num_samples, num_params, lat, lon)\n",
        "  # with num_params = 2\n",
        "  # combine two xarray data arrays to one numpy array\n",
        "  combined_arr = np.stack((t2m, tp), axis=1)\n",
        "\n",
        "  num_samples = combined_arr.shape[0]\n",
        "\n",
        "  combined_arr[np.isnan(combined_arr)] = 0\n",
        "\n",
        "  return combined_arr.astype(np.float32)\n",
        "\n",
        "def assemble_predictors_predictands(ds_t2m, ds_tp, ds_crop,\n",
        "                                    start_date, end_date):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      ds_t2m            xarray : input xarray dataset with temperature\n",
        "      ds_tp             xarray : input xarray dataset with precipitation\n",
        "      ds_crop           xarray : input xarray dataset with crop yields\n",
        "      start_date           str : the start date from which to extract sst\n",
        "      end_date             str : the end date\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "      Returns a tuple of the predictors (np array of temperature and precipitation anomalies)\n",
        "      and the predictands (np array of crop yields).\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  X = assemble_predictors(ds_t2m, ds_tp, start_date, end_date)\n",
        "\n",
        "  cropyields = ds_crop.sel(time=slice(start_date, end_date))\n",
        "\n",
        "  # convert xarray data array to numpy array\n",
        "  cropyields = cropyields.to_numpy()\n",
        "  cropyields[np.isnan(cropyields)] = 0\n",
        "  y = cropyields\n",
        "\n",
        "  return X.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "\n",
        "class ERA5Dataset(Dataset):\n",
        "    def __init__(self, predictors, predictands):\n",
        "        self.predictors = predictors\n",
        "        self.predictands = predictands\n",
        "        assert self.predictors.shape[0] == self.predictands.shape[0], \\\n",
        "               \"The number of predictors must equal the number of predictands!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.predictors.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.predictors[idx], self.predictands[idx]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "_EE02i6wD_9L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a Simple Convolutional Neural Network to model crop yields\n",
        "\n",
        "Let's define a simple convolutional neural network architecture.  This architecture has two convolutional layers followed by two transposed convolutional layers.  The output of the final layer is a 2-D array, since we are trying to model one value for each pixel: the crop yield."
      ],
      "metadata": {
        "id": "XrEZRMItafsC"
      },
      "id": "XrEZRMItafsC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the network"
      ],
      "metadata": {
        "id": "hbUd0zDwfEBG"
      },
      "id": "hbUd0zDwfEBG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7g4htyetytL"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_params=2, print_feature_dimension=False):\n",
        "        \"\"\"\n",
        "        inputs\n",
        "        -------\n",
        "            num_params                  (int) : the number of parameters\n",
        "                                                in the predictor\n",
        "            print_feature_dimension    (bool) : whether or not to print\n",
        "                                                out the dimension of the features\n",
        "                                                extracted from the conv layers\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_params, 16, 3)\n",
        "        self.conv2 = nn.Conv2d(16, 64, 3)\n",
        "        self.deconv1 = nn.ConvTranspose2d(64, 16, 3)\n",
        "        self.deconv2 = nn.ConvTranspose2d(16, 1, 3)\n",
        "        self.print_layer = Print()\n",
        "        self.print_feature_dimension = print_feature_dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        if self.print_feature_dimension:\n",
        "          x = self.print_layer(x)\n",
        "        return x\n",
        "\n",
        "class Print(nn.Module):\n",
        "    \"\"\"\n",
        "    This class prints out the size of the features\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        print(x.size())\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "S7g4htyetytL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's define a method that trains our neural network."
      ],
      "metadata": {
        "id": "RZb5SQqXbwyX"
      },
      "id": "RZb5SQqXbwyX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIgp39lRsGoW"
      },
      "source": [
        "#import numpy.ma as ma\n",
        "\n",
        "def train_network(net, criterion, optimizer, trainloader, testloader,\n",
        "                  experiment_name, num_epochs=40):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      net               (nn.Module)   : the neural network architecture\n",
        "      criterion         (nn)          : the loss function (i.e. root mean squared error)\n",
        "      optimizer         (torch.optim) : the optimizer to use update the neural network\n",
        "                                        architecture to minimize the loss function\n",
        "      trainloader       (torch.utils.data.DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the train dataset\n",
        "      testloader        (torch.utils.data. DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the test dataset\n",
        "  outputs\n",
        "  -------\n",
        "      predictions (np.array), and saves the trained neural network as a .pt file\n",
        "  \"\"\"\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  net = net.to(device)\n",
        "  best_loss = np.infty\n",
        "  train_losses, test_losses = [], []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for mode, data_loader in [('train', trainloader), ('test', testloader)]:\n",
        "      # Set the model to train mode to allow its weights to be updated\n",
        "      # while training\n",
        "      if mode == 'train':\n",
        "        net.train()\n",
        "\n",
        "      # Set the model to eval model to prevent its weights from being updated\n",
        "      # while testing\n",
        "      elif mode == 'test':\n",
        "        net.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(data_loader):\n",
        "          # get a mini-batch of predictors and predictands\n",
        "          batch_predictors, batch_predictands = data\n",
        "          training_mask = batch_predictands > 0\n",
        "          batch_predictands_masked = batch_predictands * training_mask\n",
        "          batch_predictors_masked = batch_predictors * training_mask\n",
        "\n",
        "          batch_predictands = batch_predictands_masked.to(device)\n",
        "          batch_predictors = batch_predictors_masked.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # calculate the predictions of the current neural network\n",
        "          predictions = net(batch_predictors).squeeze()\n",
        "\n",
        "          # quantify the quality of the predictions using a\n",
        "          # loss function (aka criterion) that is differentiable\n",
        "          loss = criterion(predictions, batch_predictands)\n",
        "\n",
        "          if mode == 'train':\n",
        "            # the 'backward pass: calculates the gradients of each weight\n",
        "            # of the neural network with respect to the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # the optimizer updates the weights of the neural network\n",
        "            # based on the gradients calculated above and the choice\n",
        "            # of optimization algorithm\n",
        "            optimizer.step()\n",
        "\n",
        "          # Save the model weights that have the best performance!\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      if running_loss < best_loss and mode == 'test':\n",
        "          best_loss = running_loss\n",
        "          torch.save(net, '{}.pt'.format(experiment_name))\n",
        "      print('{} Set: Epoch {:02d}. loss: {:3f}'.format(mode, epoch+1, \\\n",
        "                                            running_loss/len(data_loader)))\n",
        "      if mode == 'train':\n",
        "          train_losses.append(running_loss/len(data_loader))\n",
        "      else:\n",
        "          test_losses.append(running_loss/len(data_loader))\n",
        "\n",
        "  return \"{}.pt\".format(experiment_name), train_losses, test_losses\n",
        "\n",
        "  net = torch.load('{}.pt'.format(experiment_name))\n",
        "  net.eval()\n",
        "  net.to(device)\n",
        "\n",
        "  # the remainder of this function calculates the predictions of the best\n",
        "  # saved model\n",
        "  #predictions = np.asarray([])\n",
        "  predictions = []\n",
        "  for i, data in enumerate(testloader):\n",
        "    batch_predictors, batch_predictands = data\n",
        "    batch_predictands = batch_predictands.to(device)\n",
        "    batch_predictors = batch_predictors.to(device)\n",
        "\n",
        "    batch_predictions = net(batch_predictors).squeeze()\n",
        "    # Edge case: if there is 1 item in the batch, batch_predictions becomes a float\n",
        "    # not a Tensor. the if statement below converts it to a Tensor\n",
        "    # so that it is compatible with np.concatenate\n",
        "    if len(batch_predictions.size()) == 0:\n",
        "      batch_predictions = torch.Tensor([batch_predictions])\n",
        "    predictions.append(batch_predictions.detach().cpu().numpy())\n",
        "  return predictions, train_losses, test_losses\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "JIgp39lRsGoW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A method to use the trained network for predictions (inference)."
      ],
      "metadata": {
        "id": "4gaAM59ALCSU"
      },
      "id": "4gaAM59ALCSU"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model_name, testloader):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      model_name        (string)      : filename of saved model\n",
        "      testloader        (torch.utils.data. DataLoader): dataloader that loads the\n",
        "                                        predictors for the test dataset\n",
        "  outputs\n",
        "  -------\n",
        "      predictions (np.array)\n",
        "  \"\"\"\n",
        "\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  net = torch.load(model_name)\n",
        "  net.eval()\n",
        "  net.to(device)\n",
        "\n",
        "  # the remainder of this function calculates the predictions of the best\n",
        "  # saved model\n",
        "  #predictions = np.asarray([])\n",
        "  predictions = None\n",
        "  for i, data in enumerate(testloader):\n",
        "    batch_predictors, batch_predictands = data\n",
        "    batch_predictands = batch_predictands.to(device)\n",
        "    batch_predictors = batch_predictors.to(device)\n",
        "\n",
        "    batch_predictions = net(batch_predictors).squeeze()\n",
        "    # Edge case: if there is 1 item in the batch, batch_predictions becomes a float\n",
        "    # not a Tensor. the if statement below converts it to a Tensor\n",
        "    # so that it is compatible with np.concatenate\n",
        "    if len(batch_predictions.size()) == 0:\n",
        "      batch_predictions = torch.Tensor([batch_predictions])\n",
        "    if predictions is None:\n",
        "     predictions = batch_predictions.detach().cpu().numpy()\n",
        "    else:\n",
        "      predictions = np.concatenate([predictions, batch_predictions.detach().cpu().numpy()])\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "jtRr7Jzv_oyr"
      },
      "id": "jtRr7Jzv_oyr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actual training"
      ],
      "metadata": {
        "id": "MBVvnICufT0y"
      },
      "id": "MBVvnICufT0y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for the network and train it.\n",
        "\n",
        "The earliest possible start date is 2000-01-01, the latest possible end date is 2015-12-31."
      ],
      "metadata": {
        "id": "EnZDaZBxde_d"
      },
      "id": "EnZDaZBxde_d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USvje5kouCGb"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Assemble numpy arrays corresponding to predictors and predictands\n",
        "train_start_date = '2000-01-01'\n",
        "train_end_date = '2011-12-31'\n",
        "\n",
        "test_start_date = '2012-01-01'\n",
        "test_end_date = '2015-12-31'\n",
        "\n",
        "train_predictors, train_predictands = assemble_predictors_predictands(ds_t2m_anom_year_interp, ds_tp_anom_year_interp, ds_cropyield,\n",
        "                                                                      train_start_date, train_end_date)\n",
        "\n",
        "print(\"train_predictors: %d\" % train_predictors.shape[0])\n",
        "print(\"train_predictands: %d\" % train_predictands.shape[0])\n",
        "\n",
        "test_predictors, test_predictands = assemble_predictors_predictands(ds_t2m_anom_year_interp, ds_tp_anom_year_interp, ds_cropyield,\n",
        "                                                                    test_start_date, test_end_date)\n",
        "\n",
        "print(\"test_predictors: %d\" % test_predictors.shape[0])\n",
        "print(\"test_predictands: %d\" % test_predictands.shape[0])\n",
        "\n",
        "# Convert the numpy ararys into ERA5Dataset, which is a subset of the\n",
        "# torch.utils.data.Dataset class.  This class is compatible with\n",
        "# the torch dataloader, which allows for data loading for a CNN\n",
        "train_dataset = ERA5Dataset(train_predictors, train_predictands)\n",
        "test_dataset = ERA5Dataset(test_predictors, test_predictands)\n",
        "\n",
        "# Create a torch.utils.data.DataLoader from the ERA5Dataset() created earlier!\n",
        "# the similarity between the name DataLoader and Dataset in the pytorch API is unfortunate...\n",
        "trainloader = DataLoader(train_dataset, batch_size=2)\n",
        "testloader = DataLoader(test_dataset, batch_size=2)\n",
        "net = CNN(num_params=2, print_feature_dimension=False)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "# train the model and make predictions for the test time period\n",
        "experiment_name = \"simpleCNN_{}_{}\".format(train_start_date, train_end_date)\n",
        "model_name, train_losses, test_losses = train_network(net, nn.MSELoss(),\n",
        "                  optimizer, trainloader, testloader, experiment_name, num_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "USvje5kouCGb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results of NN training"
      ],
      "metadata": {
        "id": "IAs-MbrXfX_Y"
      },
      "id": "IAs-MbrXfX_Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot train and test losses:"
      ],
      "metadata": {
        "id": "dkzL7lZCb5by"
      },
      "id": "dkzL7lZCb5by"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Performance of {} Neural Network During Training'.format(experiment_name))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pg6OyWkhb8Mf"
      },
      "id": "Pg6OyWkhb8Mf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If test loss does not look satisfactory, try reducing the number of parameters of the network. You could define your own network architecture, which uses a different number of parameters.\n",
        "\n",
        "Alternatively, try increasing the number of training samples by using a longer time period or a larger area."
      ],
      "metadata": {
        "id": "JvC0xHkzcIbD"
      },
      "id": "JvC0xHkzcIbD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create predictions"
      ],
      "metadata": {
        "id": "Pay8XjXvA8PC"
      },
      "id": "Pay8XjXvA8PC"
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(model_name, testloader)"
      ],
      "metadata": {
        "id": "xXs7K-InA-Vw"
      },
      "id": "xXs7K-InA-Vw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation and root mean squared error:"
      ],
      "metadata": {
        "id": "hU6bkIQGc5nL"
      },
      "id": "hU6bkIQGc5nL"
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictands_1d = test_predictands.reshape(4 * 361 * 421)\n",
        "predictions_1d = predictions.reshape(4 * 361 * 421)\n",
        "corr, _ = pearsonr(test_predictands_1d, predictions_1d)\n",
        "#corr, _ = pearsonr(test_predictands, predictions)\n",
        "\n",
        "rmse = mean_squared_error(test_predictands_1d, predictions_1d) ** 0.5\n",
        "print(corr)\n",
        "print(\"RMSE: {:.2f}\".format(rmse))\n"
      ],
      "metadata": {
        "id": "iu2GlMiQcaXj"
      },
      "id": "iu2GlMiQcaXj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show observations and predictions:"
      ],
      "metadata": {
        "id": "Wkgxzkc8Aypd"
      },
      "id": "Wkgxzkc8Aypd"
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12, 12))\n",
        "# observations\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "plot1 = ax1.imshow(test_predictands[0, :, :])\n",
        "ax1.title.set_text(\"observed\")\n",
        "ax1_axes = plot1.axes\n",
        "ax1_axes.invert_yaxis()\n",
        "# predictions\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "plot2 = ax2.imshow(predictions[0, :, :])\n",
        "ax2.title.set_text(\"predicted\")\n",
        "ax2_axes = plot2.axes\n",
        "ax2_axes.invert_yaxis()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W5l8DtaNXMFq"
      },
      "id": "W5l8DtaNXMFq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Improvements and limitations**\n",
        "\n",
        "The most important limitation here is the small amount of training data available from Eurostat. Unfortunately, this can not be changed.\n",
        "\n",
        "Possible improvements could be\n",
        "* tiling of the training data to e.g. 128x128 or 256x256 pixels, but this would make the code in this notebook more complex than it alread is\n",
        "* masking out oceans and inland waterbodies in the predictions because crops are not planted in these areas and the results are thus confusing / unrealistic\n"
      ],
      "metadata": {
        "id": "CZBFFP5Jbk_r"
      },
      "id": "CZBFFP5Jbk_r"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}